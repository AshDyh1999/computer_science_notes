{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms\n",
    "\n",
    "### explantory variables\n",
    "independent variables\n",
    "### Collinearity\n",
    "a linear association between two explantory variables: $X_{2i} = \\lambda_0 + \\lambda_1 X_{1i} $\n",
    "\n",
    "### Multicollinearity\n",
    "two or more explantory variables are highly linearly related: $\\lambda_0 + \\lambda_1 X_{1i} + \\lambda_2 X_{2i} + \\dots + \\lambda_k X_{ki} = 0$\n",
    "\n",
    "\n",
    "### degree of freedom\n",
    "The minimum number of independent coordinates that can specify the position of the system completely.\n",
    "\n",
    "See examples in the \"Of random vectors\" section from [Degrees of freedom(statistics)](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))\n",
    "\n",
    "### singular matrix\n",
    "A square matrix that does not have a matrix inverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "lower bias, higher variance\n",
    "\n",
    "$Y_i = w_0 + w_1 X_{1i} + \\dots + w_k X_{ki}$\n",
    "\n",
    "$X^T X$, where $$X = \\begin{bmatrix}\n",
    "    1       & X_{11} & \\ldots & X_{k1} \\\\\n",
    "    \\vdots  & \\vdots &        & \\vdots \\\\\n",
    "    1       & X_{1N} & \\ldots & X_{kN} \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "$k$: number of explantory variables\n",
    "\n",
    "$N$: number of observations. ($N \\geq k + 1$)\n",
    "\n",
    "Details: [最小二乘线性回归从入门到放弃 :-)](https://www.bilibili.com/video/av13759873)\n",
    "\n",
    "If $X$ has full column rank, $w$  has an explicit solution in matrix form as:$$w^* = (X^T X)^{-1} X^T y$$\n",
    "Then, the fitted values by OLS will be: $$y = X w^*$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[[ 1.  1.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 1. -1.  1.]]\n",
      "w_opt\n",
      "[ 2.  3. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x1, x2):\n",
    "    return 2 +  3 * x1 - x2\n",
    "\n",
    "x1 = [1, 3, 0, 1, -1]\n",
    "x2 = [2, 4, 1, 0, 1]\n",
    "y = [f(x11, x22) for x11, x22 in zip(x1, x2)]\n",
    "\n",
    "X = np.zeros((5, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = x1\n",
    "X[:, 2] = x2\n",
    "print('X', X, sep='\\n')\n",
    "\n",
    "w_opt = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T ), y)\n",
    "print('w_opt', w_opt, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Fix the issue when $X^T X$ is singular or nearly singular: Add a small constant **positive** value $\\lambda$ to the diagonl entries of the matrix $X^T X$ before taking its inverse. [Regularization Part 1: Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "\n",
    "\n",
    "+ high bias, low variance\n",
    "+ make $y$ less sensitive to `x` by decreasing the slope\n",
    "+ smaller sample size (Note that OLS can only be applied to dataset whose size is not less than its variables, while Ridge Regression has no such constraint) => improve the generalization ability of the model\n",
    "\n",
    "The ridge estimator is $$\\beta^{ridge} = (X^T X + \\lambda I_p)^{-1} X^T y$$\n",
    "\n",
    "Note that OLS is $$\\beta^{ols} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "The ridge estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lVert \\beta \\rVert^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Similar to Ridge Regression, except that its penality is $\\lambda \\lvert \\beta \\rvert$. [Regularization Part 2: Lasso Regression\n",
    "](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "\n",
    "+ Ridge Regression does not exclude useless variables, while Lasso Regression does by setting their coefficients to zero\n",
    "\n",
    "\n",
    "The lasso estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lvert \\beta \\rvert$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "Ridge + Lasso. [Regularization Part 3: Elastic Net Regression](https://www.youtube.com/watch?v=1dKRdX9bfIo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
