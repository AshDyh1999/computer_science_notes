{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms\n",
    "\n",
    "### explantory variables\n",
    "independent variables\n",
    "### Collinearity\n",
    "a linear association between two explantory variables: $X_{2i} = \\lambda_0 + \\lambda_1 X_{1i} $\n",
    "\n",
    "### Multicollinearity\n",
    "two or more explantory variables are highly linearly related: $\\lambda_0 + \\lambda_1 X_{1i} + \\lambda_2 X_{2i} + \\dots + \\lambda_k X_{ki} = 0$\n",
    "\n",
    "\n",
    "### degree of freedom\n",
    "The minimum number of independent coordinates that can specify the position of the system completely.\n",
    "\n",
    "See examples in the \"Of random vectors\" section from [Degrees of freedom(statistics)](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics))\n",
    "\n",
    "### singular matrix\n",
    "A square matrix that does not have a matrix inverse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "lower bias, higher variance\n",
    "\n",
    "$Y_i = w_0 + w_1 X_{1i} + \\dots + w_k X_{ki}$\n",
    "\n",
    "$X^T X$, where $$X = \\begin{bmatrix}\n",
    "    1       & X_{11} & \\ldots & X_{k1} \\\\\n",
    "    \\vdots  & \\vdots &        & \\vdots \\\\\n",
    "    1       & X_{1N} & \\ldots & X_{kN} \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "$k$: number of explantory variables\n",
    "\n",
    "$N$: number of observations. ($N \\geq k + 1$)\n",
    "\n",
    "Details: [最小二乘线性回归从入门到放弃 :-)](https://www.bilibili.com/video/av13759873)\n",
    "\n",
    "If $X$ has full column rank, $w$  has an explicit solution in matrix form as:$$w^* = (X^T X)^{-1} X^T y$$\n",
    "Then, the fitted values by OLS will be: $$y = X w^*$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[[ 1.  1.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 1. -1.  1.]]\n",
      "w_opt\n",
      "[ 2.  3. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x1, x2):\n",
    "    return 2 +  3 * x1 - x2\n",
    "\n",
    "x1 = [1, 3, 0, 1, -1]\n",
    "x2 = [2, 4, 1, 0, 1]\n",
    "y = [f(x11, x22) for x11, x22 in zip(x1, x2)]\n",
    "\n",
    "X = np.zeros((5, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = x1\n",
    "X[:, 2] = x2\n",
    "print('X', X, sep='\\n')\n",
    "\n",
    "w_opt = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T ), y)\n",
    "print('w_opt', w_opt, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Fix the issue when $X^T X$ is singular or nearly singular: Add a small constant **positive** value $\\lambda$ to the diagonl entries of the matrix $X^T X$ before taking its inverse. [Regularization Part 1: Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "\n",
    "\n",
    "+ high bias, low variance\n",
    "+ make $y$ less sensitive to `x` by decreasing the slope\n",
    "+ smaller sample size (Note that OLS can only be applied to dataset whose size is not less than its variables, while Ridge Regression has no such constraint) => improve the generalization ability of the model\n",
    "\n",
    "The ridge estimator is $$\\beta^{ridge} = (X^T X + \\lambda I_p)^{-1} X^T y$$\n",
    "\n",
    "Note that OLS is $$\\beta^{ols} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "The ridge estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lVert \\beta \\rVert^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Similar to Ridge Regression, except that its penality is $\\lambda \\lvert \\beta \\rvert$. [Regularization Part 2: Lasso Regression\n",
    "](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "\n",
    "+ Ridge Regression does not exclude useless variables, while Lasso Regression does by setting their coefficients to zero\n",
    "\n",
    "\n",
    "The lasso estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lvert \\beta \\rvert$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "Ridge + Lasso. [Regularization Part 3: Elastic Net Regression](https://www.youtube.com/watch?v=1dKRdX9bfIo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "Pytorch: [BatchNorm2d](https://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm1d)\n",
    "\n",
    "Normalize each dimension(such as `x[:, k, :, :]` in CHW version):\n",
    "$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\textbf{E}[x^{(k)}]}{\\sqrt{  \\text{Var}[x^{(k)}] + \\epsilon   }}$$\n",
    "\n",
    "Scale and shift the normalized value:\n",
    "$$ y^{(k)} = \\gamma ^ {(k)} \\hat{x} ^{(k)} + \\beta^{(k)} $$\n",
    "\n",
    "For pytorch, $\\gamma$ and $\\beta$ are stored in `bn.weight` and `bn.bias` respectively. If `affine` is `False`, `bn` has no learnable parameters and thus `bn.weight` and `bn.bias` are `None`.\n",
    "\n",
    "```\n",
    "running_mean = (1 - momentum) * running_mean + momentum * batch_mean\n",
    "running_var  = (1 - momentum) * running_var  + momentum * batch_var\n",
    "```\n",
    "\n",
    "Note that `torch.var` is default to unbiased, which means that it is  $Var[x] = \\frac{m}{m - 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn.eps: 1e-05\n",
      "y:\n",
      "tensor([[[[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]]]])\n",
      "y_mine:\n",
      "tensor([[[[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]],\n",
      "\n",
      "         [[-1.1963, -1.1138],\n",
      "          [-1.0313, -0.9488],\n",
      "          [-0.8663, -0.7838]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]],\n",
      "\n",
      "         [[ 0.7838,  0.8663],\n",
      "          [ 0.9488,  1.0313],\n",
      "          [ 1.1138,  1.1963]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.arange(48).reshape(2, 4, 3, 2).float()\n",
    "bn = torch.nn.BatchNorm2d(4, affine=False)\n",
    "print('bn.eps:', bn.eps)\n",
    "y = bn(x)\n",
    "\n",
    "y_mine = torch.zeros_like(y)\n",
    "for i in range(4):\n",
    "    y_mine[:, i, :, :] = (x[:, i, :, :] - torch.mean(x[:, i, :, :])) / torch.sqrt(torch.var(x[:, i, :, :], unbiased=False) + bn.eps)\n",
    "\n",
    "print('y:', y, sep='\\n')\n",
    "print('y_mine:', y_mine, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn.bias: tensor([5., 5., 5., 5.])\n",
      "y:\n",
      "tensor([[[[3.8226, 3.9038],\n",
      "          [3.9850, 4.0662],\n",
      "          [4.1474, 4.2286]],\n",
      "\n",
      "         [[4.6951, 4.7161],\n",
      "          [4.7371, 4.7582],\n",
      "          [4.7792, 4.8002]],\n",
      "\n",
      "         [[4.4152, 4.4555],\n",
      "          [4.4959, 4.5362],\n",
      "          [4.5765, 4.6168]],\n",
      "\n",
      "         [[4.2404, 4.2928],\n",
      "          [4.3452, 4.3975],\n",
      "          [4.4499, 4.5023]]],\n",
      "\n",
      "\n",
      "        [[[5.7714, 5.8526],\n",
      "          [5.9338, 6.0150],\n",
      "          [6.0962, 6.1774]],\n",
      "\n",
      "         [[5.1998, 5.2208],\n",
      "          [5.2418, 5.2629],\n",
      "          [5.2839, 5.3049]],\n",
      "\n",
      "         [[5.3832, 5.4235],\n",
      "          [5.4638, 5.5041],\n",
      "          [5.5445, 5.5848]],\n",
      "\n",
      "         [[5.4977, 5.5501],\n",
      "          [5.6025, 5.6548],\n",
      "          [5.7072, 5.7596]]]], grad_fn=<ThnnBatchNormBackward>)\n",
      "y_mine:\n",
      "tensor([[[[3.8226, 3.9038],\n",
      "          [3.9850, 4.0662],\n",
      "          [4.1474, 4.2286]],\n",
      "\n",
      "         [[4.6951, 4.7161],\n",
      "          [4.7371, 4.7582],\n",
      "          [4.7792, 4.8002]],\n",
      "\n",
      "         [[4.4152, 4.4555],\n",
      "          [4.4959, 4.5362],\n",
      "          [4.5765, 4.6168]],\n",
      "\n",
      "         [[4.2404, 4.2928],\n",
      "          [4.3452, 4.3975],\n",
      "          [4.4499, 4.5023]]],\n",
      "\n",
      "\n",
      "        [[[5.7714, 5.8526],\n",
      "          [5.9338, 6.0150],\n",
      "          [6.0962, 6.1774]],\n",
      "\n",
      "         [[5.1998, 5.2208],\n",
      "          [5.2418, 5.2629],\n",
      "          [5.2839, 5.3049]],\n",
      "\n",
      "         [[5.3832, 5.4235],\n",
      "          [5.4638, 5.5041],\n",
      "          [5.5445, 5.5848]],\n",
      "\n",
      "         [[5.4977, 5.5501],\n",
      "          [5.6025, 5.6548],\n",
      "          [5.7072, 5.7596]]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm2d(4) # eps=1e-5, affine=True(gamma and beta are enabled)\n",
    "bn.bias[:] += 5 # set to 5\n",
    "print('bn.bias:', bn.bias.data)\n",
    "y = bn(x)\n",
    "\n",
    "\n",
    "y_mine = torch.zeros_like(y)\n",
    "for i in range(4):\n",
    "    y_mine[:, i, :, :] = (x[:, i, :, :] - torch.mean(x[:, i, :, :])) / torch.sqrt(torch.var(x[:, i, :, :], unbiased=False) + bn.eps) * bn.weight[i] + bn.bias[i]\n",
    "\n",
    "print('y:', y, sep='\\n')\n",
    "print('y_mine:', y_mine, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "tensor([[[[ 0.0000,  0.4406],\n",
      "          [ 0.8813,  1.3219],\n",
      "          [ 1.7626,  2.2032]],\n",
      "\n",
      "         [[ 3.9458,  4.6034],\n",
      "          [ 5.2610,  5.9186],\n",
      "          [ 6.5763,  7.2339]],\n",
      "\n",
      "         [[ 8.3611,  9.0579],\n",
      "          [ 9.7547, 10.4514],\n",
      "          [11.1482, 11.8449]],\n",
      "\n",
      "         [[10.9858, 11.5961],\n",
      "          [12.2065, 12.8168],\n",
      "          [13.4271, 14.0374]]],\n",
      "\n",
      "\n",
      "        [[[10.5755, 11.0161],\n",
      "          [11.4568, 11.8974],\n",
      "          [12.3381, 12.7787]],\n",
      "\n",
      "         [[19.7288, 20.3864],\n",
      "          [21.0440, 21.7017],\n",
      "          [22.3593, 23.0169]],\n",
      "\n",
      "         [[25.0834, 25.7802],\n",
      "          [26.4769, 27.1737],\n",
      "          [27.8705, 28.5672]],\n",
      "\n",
      "         [[25.6336, 26.2439],\n",
      "          [26.8542, 27.4645],\n",
      "          [28.0749, 28.6852]]]], grad_fn=<ThnnBatchNormBackward>)\n",
      "y_mine:\n",
      "tensor([[[[ 0.0000,  0.4406],\n",
      "          [ 0.8813,  1.3219],\n",
      "          [ 1.7626,  2.2032]],\n",
      "\n",
      "         [[ 3.9458,  4.6034],\n",
      "          [ 5.2610,  5.9186],\n",
      "          [ 6.5763,  7.2339]],\n",
      "\n",
      "         [[ 8.3611,  9.0579],\n",
      "          [ 9.7547, 10.4514],\n",
      "          [11.1482, 11.8449]],\n",
      "\n",
      "         [[10.9858, 11.5961],\n",
      "          [12.2065, 12.8168],\n",
      "          [13.4271, 14.0374]]],\n",
      "\n",
      "\n",
      "        [[[10.5755, 11.0161],\n",
      "          [11.4568, 11.8974],\n",
      "          [12.3381, 12.7787]],\n",
      "\n",
      "         [[19.7288, 20.3864],\n",
      "          [21.0440, 21.7017],\n",
      "          [22.3593, 23.0169]],\n",
      "\n",
      "         [[25.0834, 25.7802],\n",
      "          [26.4769, 27.1737],\n",
      "          [27.8705, 28.5672]],\n",
      "\n",
      "         [[25.6336, 26.2439],\n",
      "          [26.8542, 27.4645],\n",
      "          [28.0749, 28.6852]]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm2d(4).eval() # eps=1e-5, affine=True(gamma and beta are enabled)\n",
    "y = bn(x)\n",
    "\n",
    "y_mine = torch.zeros_like(y)\n",
    "for i in range(4):\n",
    "    y_mine[:, i, :, :] = (x[:, i, :, :] - bn.running_mean[i]) / torch.sqrt(bn.running_var[i] + bn.eps) * bn.weight[i] + bn.bias[i]\n",
    "    \n",
    "print('y:', y, sep='\\n')\n",
    "print('y_mine:', y_mine, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4500, 2.0500, 2.6500, 3.2500])\n",
      "y:\n",
      "tensor([[[[-0.1553, -0.0482],\n",
      "          [ 0.0589,  0.1660],\n",
      "          [ 0.2731,  0.3802]],\n",
      "\n",
      "         [[ 0.6314,  0.7912],\n",
      "          [ 0.9511,  1.1109],\n",
      "          [ 1.2707,  1.4306]],\n",
      "\n",
      "         [[ 1.5835,  1.7528],\n",
      "          [ 1.9222,  2.0915],\n",
      "          [ 2.2609,  2.4302]],\n",
      "\n",
      "         [[ 2.1881,  2.3364],\n",
      "          [ 2.4848,  2.6331],\n",
      "          [ 2.7814,  2.9298]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4152,  2.5223],\n",
      "          [ 2.6294,  2.7365],\n",
      "          [ 2.8436,  2.9507]],\n",
      "\n",
      "         [[ 4.4675,  4.6274],\n",
      "          [ 4.7872,  4.9471],\n",
      "          [ 5.1069,  5.2668]],\n",
      "\n",
      "         [[ 5.6479,  5.8173],\n",
      "          [ 5.9866,  6.1560],\n",
      "          [ 6.3253,  6.4947]],\n",
      "\n",
      "         [[ 5.7483,  5.8967],\n",
      "          [ 6.0450,  6.1933],\n",
      "          [ 6.3417,  6.4900]]]], grad_fn=<ThnnBatchNormBackward>)\n",
      "y_mine:\n",
      "tensor([[[[-0.1553, -0.0482],\n",
      "          [ 0.0589,  0.1660],\n",
      "          [ 0.2731,  0.3802]],\n",
      "\n",
      "         [[ 0.6314,  0.7912],\n",
      "          [ 0.9511,  1.1109],\n",
      "          [ 1.2707,  1.4306]],\n",
      "\n",
      "         [[ 1.5835,  1.7528],\n",
      "          [ 1.9222,  2.0915],\n",
      "          [ 2.2609,  2.4302]],\n",
      "\n",
      "         [[ 2.1881,  2.3364],\n",
      "          [ 2.4848,  2.6331],\n",
      "          [ 2.7814,  2.9298]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4152,  2.5223],\n",
      "          [ 2.6294,  2.7365],\n",
      "          [ 2.8436,  2.9507]],\n",
      "\n",
      "         [[ 4.4676,  4.6274],\n",
      "          [ 4.7872,  4.9471],\n",
      "          [ 5.1069,  5.2668]],\n",
      "\n",
      "         [[ 5.6479,  5.8173],\n",
      "          [ 5.9866,  6.1560],\n",
      "          [ 6.3253,  6.4947]],\n",
      "\n",
      "         [[ 5.7483,  5.8967],\n",
      "          [ 6.0450,  6.1933],\n",
      "          [ 6.3417,  6.4900]]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "bn = bn.train()\n",
    "bn(x)\n",
    "print(bn.running_mean)\n",
    "bn = bn.eval()\n",
    "\n",
    "y = bn(x)\n",
    "\n",
    "y_mine = torch.zeros_like(y)\n",
    "for i in range(4):\n",
    "    y_mine[:, i, :, :] = (x[:, i, :, :] - bn.running_mean[i]) / torch.sqrt(bn.running_var[i] + bn.eps) * bn.weight[i] + bn.bias[i]\n",
    "    \n",
    "print('y:', y, sep='\\n')\n",
    "print('y_mine:', y_mine, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0],\n",
       "          [0, 1],\n",
       "          [1, 1]],\n",
       "\n",
       "         [[1, 1],\n",
       "          [0, 1],\n",
       "          [1, 1]],\n",
       "\n",
       "         [[0, 0],\n",
       "          [1, 1],\n",
       "          [1, 1]],\n",
       "\n",
       "         [[1, 1],\n",
       "          [1, 1],\n",
       "          [0, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1],\n",
       "          [0, 1],\n",
       "          [0, 1]],\n",
       "\n",
       "         [[0, 1],\n",
       "          [0, 1],\n",
       "          [0, 1]],\n",
       "\n",
       "         [[1, 0],\n",
       "          [1, 1],\n",
       "          [1, 0]],\n",
       "\n",
       "         [[1, 0],\n",
       "          [1, 1],\n",
       "          [1, 0]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y == y_mine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
